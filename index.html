<!doctype html>
<!-- the code uses peachananr's tiltedpage_scroll 
visit https://github.com/peachananr for more design
-->
<html>
<head>
  <meta name="viewport" content="width=device-width, min-width=1024px" />
  <title>CS349 Project: Video Event Detection - Northwestern University</title>
  <link href="css/bootstrap.min.css" rel="stylesheet" media="screen"> 

	<script type="text/javascript" src="http://code.jquery.com/jquery-1.9.1.js"></script>
  <script type="text/javascript" src="jquery.tiltedpage-scroll.js"></script>
  <link href='tiltedpage-scroll.css' rel='stylesheet' type='text/css'>
  <style>
body
{
/*background-color : #EFEFEF;*/
    background-image: url("bg.jpg");
    -webkit-background-size: cover;
  -moz-background-size: cover;
  -o-background-size: cover;
  background-size: cover;
  -ms-filter: "progid:DXImageTransform.Microsoft.AlphaImageLoader(src='http://media02.hongkiat.com/oversized-background-image-design/bg.jpg', sizingMethod='scale')";
  filter: progid:DXImageTransform.Microsoft.AlphaImageLoader(src='.http://media02.hongkiat.com/oversized-background-image-design/bg.jpg', sizingMethod='scale');
  height:100%;
  position: relative;
}

.main
{
	float: center;
	width: 100%;
	margin: 0 auto;
	margin-bottom: 350px;
	overflow: hidden;
}

.content
{
	float: center;
	width: 60%;
	min-width: 650px;
	zoom: auto;
	/*overflow:auto;*/
	max-height:630px;
	max-width: 1024px;
    /* width : 800px; */
    padding : 25px 50px;
    margin : 25px auto;
    background-color : #fff;
    box-shadow: 0px 0px 10px #999;
    border-radius: 15px;
    position: relative;
}

a, a:visited
{
    color : blue;
}

#authors
{
    text-align : center;
    margin-bottom : 20px;
}

#conference
{
    text-align : center;
    margin-bottom : 20px;
    font-style : italic;
}

#authors a
{
    margin : 0 10px;
}

.author
{
    margin : 0 10px;
}

h1
{
    text-align : center;
    font-family : Arial;
    font-size : 30px;
}

.downloadpaper
{
    padding-left : 20px;
    float : right;
    text-align : center;
}

.downloadpaper a
{
    font-weight : bold;
    text-align : center;
}

.conf-mat table {
  border-spacing: 0.5rem;
}
.conf-mat td {
  padding: 0.5rem;
  background: hsl(200, 99%, 50%);
   /*display: inline-block;
  position: relative;
  -webkit-transform: translateZ(0);
  -ms-transform: translateZ(0);
  transform: translateZ(0);*/
  box-shadow: 0 0 1px rgba(0, 0, 0, 0);
}

.outline-inward {
  display: block;
  position: relative;
  -webkit-transform: translateZ(0);
  -ms-transform: translateZ(0);
  transform: translateZ(0);
  box-shadow: 0 0 1px rgba(0, 0, 0, 0);
}

.outline-inward:before {
  pointer-events: none;
  content: '';
  position: absolute;
  border: #e1e1e1 solid 4px;
  top: -16px;
  right: -16px;
  bottom: -16px;
  left: -16px;
  opacity: 0;
  -webkit-transition-duration: .3s;
  transition-duration: .3s;
  -webkit-transition-property: top, right, bottom, left;
  transition-property: top, right, bottom, left;
}

.outline-inward:hover:before {
  top: -8px;
  right: -8px;
  bottom: -8px;
  left: -8px;
  opacity: 1;
}
.conf-mat .header {background: #ffffff; font-weight: bold}
.conf-mat .td11 { background: hsl(99, 98%, 51%); }
.conf-mat .td10 { background: hsl(103, 88%, 47%); }
.conf-mat .td12 { background: hsl(130, 91%, 41%); }
.conf-mat .td9 { background: hsl(150, 100%, 50%); }
.td3 { background: hsl(170, 70%, 50%); }
.conf-mat .td2 { background: hsl(180, 80%, 50%); }
.conf-mat .td1 { background: hsl(190, 90%, 50%); }

</style>
	<script>
	  $(document).ready(function(){
      $(".main").tiltedpage_scroll({
        angle: 20
      });
		});
		
	  </script>
    <link rel="stylesheet" href="../css/demo.css">
    <script src="../js/demo.js"></script>
  </head>
  <body>
 
  	  <div class="main">
  	  <section class="page1">
        <div class="content">
        <h1>CS349 Project: Video Event Detection</h1>
        <p id="authors">
        <a href="mailto:zywang@u.northwestern.edu">Zhiyuan Wang</a>
        <span class="author"> Haodong Wang </span>
        <span class="author"> Xi Zheng </span>
        </p>
        <center><p> Northwestern University </p> </center>
        <center><p> Instructor: <a href="http://www.cs.northwestern.edu/~ddowney">Doug Downey </a> </p> </center>
        </p>

        <div class="downloadpaper">
        <a href="final.pdf"><img src="cover.png" width="200px"><br><br>View Report</a>
        </div>

        <p>Our task is to detect specific event (mostly human involved) in a video based on spatial-temporal features of its visual content.</p>

        <p>Human involved events are major events and usually the events we care most in movies, web videos, surveillance video, etc. Event detection for video is an important problem in many applications, especially nowadays video become one of the most important information resource and watching videos become part of our everyday life. For example this technology can help us make index of key frames, which can be used by user to do content-based browsing (fast-forward to the moment they want to watch) [1].</p>

        <p> You can read the following part of this page or read our <a href="final.pdf">report</a> for detail. </p>

        <br clear="all">

        </div>
      </section>
  	  <section class="page2">
        <div class="content">
        <h2> Method </h2>
        <table align="center">
            <tr>
            <td align="right"> <img src="f1.png" height="65%" width="65%"> </td>
            <td align="left"> <img src="f2.png" height="65%" width="65%"> </td>
            </tr>
        </table>
        <center> Example of extracted spatial-time interest points(STIP). </center>
        <center> The figure shows that STIP can capture movement part of human.</center>
        <p> We use trackelet feature[2]. The features are 256-dimensional spatial-time interest points(STIP) based on HOG and HOF(see report for detail). Since video's temporal and spatial properties vary between each other, the number of STIP extracted from each video also varies. The first thing is to project all the feature vector of different videos into same space. We clustering these spatial-temporal interest points to learn a dictionary (finally get 891 vocabularies or bases). Then calculate histogram of projected points in a video as its feature vector. We use these feature vectors to train SVM classifier for each action. Prediction is made by one-vs-all strategy. </p>
        </div>
      </section>
  	  <section class="page3">
        <div class="content">
          <h2> Experiments </h2>
          <p> The experiments are done on KTH action dataset, which contains 6 classes of actions with almost 100 videos for each class. We totally use 72 videos for training and 72 videos for test, the number of videos for different class is similar (11 to 12). We trained SVM with different kernels and variables, and linear model achieved the best performance, 86.1% accuracy on test set. It may casused by our relatively small dataset, and the actions in videos are performed in similar way. Below is the confusion matrix. That shows good features' importance in classification, even simple classifier like linear SVM can work well with them. </p>
          <center>
          <table class="conf-mat">
              <tr>
                  <th> </th>
                  <th>box </th>
                  <th> handclap </th>
                  <th> handwave </th>
                  <th> joggling </th>
                  <th> running </th>
                  <th> walking </th>
              </tr>
              <tr>
              <td class="header"> boxing</td>
              <td class="td10 outline-inward"> 10 </td>
              <td class="td1"> 1 </td>
              <td> 0 </td>
              <td> 0 </td>
              <td> 0 </td>
              <td class="td1"> 1 </td>
              </tr>
              <tr>
              <td class="header"> handclapping</td>
              <td> 0</td>
              <td class="td12 outline-inward"> 12</td>
              <td> 0</td>
              <td> 0</td>
              <td> 0</td>
              <td> 0</td>
              </tr>
              <tr>
              <td class="header"> handwaving</td>
              <td class="td1"> 1</td>
              <td> 0</td>
              <td class="td11 outline-inward"> 11</td>
              <td> 0</td>
              <td> 0</td>
              <td> 0</td>
              </tr>
              <tr>
              <td class="header"> joggling</td>
              <td class="td1"> 1</td>
              <td> 0</td>
              <td> 0</td>
              <td class="td9 outline-inward"> 9</td>
              <td class="td2"> 2</td>
              <td> 0</td>
              </tr>
              <tr>
              <td class="header"> running </td>
              <td class="td1"> 1</td>
              <td> 0</td>
              <td> 0</td>
              <td class="td2"> 2</td>
              <td class="td9 outline-inward"> 9</td>
              <td> 0</td>
              </tr>
              <tr>
              <td class="header"> walking </td>
              <td class="td1"> 1</td>
              <td> 0</td>
              <td> 0</td>
              <td> 0</td>
              <td> 0</td>
              <td class="td11 outline-inward"> 11</td>
              </tr>
          </table>
          </center>
          </div>
      </section>
  	  <section class="page4">
        <div class="content">

          <h2>References</h2>
          <ol>
          <li> Laptev, I., Marszalek, M., Schmid, C., & Rozenfeld, B. (2008, June). Learning realistic human actions from movies. In <em>Computer Vision and Pattern Recognition</em>, 2008. CVPR 2008. <em>IEEE Conference</em> on (pp. 1-8). IEEE.</li>
          <li> Raptis, M., & Soatto, S. (2010). Tracklet descriptors for action modeling and video analysis. In <em>Computer Visionâ€“ECCV </em> 2010 (pp. 577-590). Springer Berlin Heidelberg. </li>
          </ol>

          </div>
      </section>
    </div>
  </div>
</body>
</html>
