<html>
<head>
<title>CS349 Project: Video Event Detection - Northwestern University</title>
<meta name="google-site-verification" content="NksNPfO4SApMtvU2rGHxr4DPan2Uy6Pz-rP9cA0k1mg" />
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="jquery.mlens-1.0.min.js"></script>
<script type="text/javascript">

var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-17813713-3']);
_gaq.push(['_setDomainName', '.mit.edu']);
_gaq.push(['_trackPageview']);

_gaq.push(['t2._setAccount', 'UA-24450278-5']);
_gaq.push(['t2._trackPageview']);

(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'stats.g.doubleclick.net/dc.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();

</script>

<style>
body
{
	/*background-color : #EFEFEF;*/
    background-image: url("bg.jpg");
    -webkit-background-size: cover;  
  -moz-background-size: cover;  
  -o-background-size: cover;  
  background-size: cover;  
  -ms-filter: "progid:DXImageTransform.Microsoft.AlphaImageLoader(src='http://media02.hongkiat.com/oversized-background-image-design/bg.jpg', sizingMethod='scale')";  
  filter: progid:DXImageTransform.Microsoft.AlphaImageLoader(src='.http://media02.hongkiat.com/oversized-background-image-design/bg.jpg', sizingMethod='scale'); 
}
.content
{
    width : 800px;
    padding : 25px 50px;
    margin : 25px auto;
    background-color : #fff;
    box-shadow: 0px 0px 10px #999;
    border-radius: 15px; 
    
}

.contentblock
{
    width : 950px;
    margin : 0 auto;
    padding : 0;
    border-spacing : 25px 0;
}

.contentblock td
{
    background-color : #fff;
    padding : 25px 50px;
    vertical-align : top;
    box-shadow: 0px 0px 10px #999;
    border-radius: 15px; 
}

a, a:visited
{
    color : blue;
}

#authors
{
    text-align : center;
    margin-bottom : 20px;
}

#conference
{
    text-align : center;
    margin-bottom : 20px;
    font-style : italic;
}

#authors a 
{
    margin : 0 10px;
}

.author
{
    margin : 0 10px;
}

h1
{
    text-align : center;
    font-family : Arial;
    font-size : 30px;
}

code
{
	display : block;
	padding : 10px;
	margin : 10px 10px;
}
p code
{
    display : inline;
    padding : 0;
    margin : 0;
}
#teasers
{
    margin : 0 auto;    
}

#teasers td
{
    margin : 0 auto;
    text-align : center;
    padding : 5px;
}

#teasers img
{
    width : 250px; 
}

#results img
{
    width : 133px;
}

#seeintodark {
    margin : 0 auto;
}

#sift 
{
    margin : 0 auto;
}

#sift img
{
    width : 250px;
}

.downloadpaper 
{
    padding-left : 20px;
    float : right;
    text-align : center;
}

.downloadpaper a 
{
    font-weight : bold;
    text-align : center;
}

#demoframe
{
    border : 0;
    padding : 0;
    margin : 0;
    width : 100%;
    height : 340px;
}

#feedbackform
{
    border : 1px solid #ccc;
    margin : 0 auto;
    border-radius : 15px;
}

.conf-mat table {
  border-spacing: 0.5rem;
}
.conf-mat td {
  padding: 0.5rem;
  background: hsl(200, 99%, 50%);
   display: inline-block;
  /*position: relative;*/
  -webkit-transform: translateZ(0);
  -ms-transform: translateZ(0);
  transform: translateZ(0);
  box-shadow: 0 0 1px rgba(0, 0, 0, 0);
}

.conf-mat td:hover {
	-webkit-filter: brightness(10%);
-webkit-transform: translate(0, -10px);
-moz-transform: translate(0, -10px);
-ms-transform: translate(0, -10px);
-o-transform: translate(0, -10px);
transform: translate(0, -10px);
}

.conf-mat td:before {
  pointer-events: none;
  content: '';
  position: absolute;
  border: #e1e1e1 solid 4px;
  top: -16px;
  right: -16px;
  bottom: -16px;
  left: -16px;
  opacity: 0;
  -webkit-transition-duration: .3s;
  transition-duration: .3s;
  -webkit-transition-property: top, right, bottom, left;
  transition-property: top, right, bottom, left;
}

.conf-mat td:hover:before {
  top: -8px;
  right: -8px;
  bottom: -8px;
  left: -8px;
  opacity: 1;
}

.conf-mat .header {background: #ffffff; font-weight: bold}
.conf-mat .td11 { background: hsl(99, 98%, 51%); }
.conf-mat .td10 { background: hsl(103, 88%, 47%); }
.conf-mat .td12 { background: hsl(130, 91%, 41%); }
.conf-mat .td9 { background: hsl(150, 100%, 50%); }
.td3 { background: hsl(170, 70%, 50%); }
.conf-mat .td2 { background: hsl(180, 80%, 50%); }
.conf-mat .td1 { background: hsl(190, 90%, 50%); }

</style>
</head>

<body>

<div class="content">
<h1>CS349 Project: Video Event Detection</h1>
<p id="authors">
<a href="mailto:zywang@u.northwestern.edu">Zhiyuan Wang</a>
<span class="author"> Haodong Wang </span>
<span class="author"> Xi Zheng </span>
</p>
<center><p> Northwestern University </p> </center>
<center><p>  Instructor: <a href="http://www.cs.northwestern.edu/~ddowney">Doug Downey </a> </p> </center>
</p>

<div class="downloadpaper">
<a href="final.pdf"><img src="papericon.png" width="200px"><br><br>View Report</a>
</div>

<p>Our task is to detect specific event (mostly human involved) in a video based on spatial-temporal features of its visual content.</p>

<p>Human involved events are major events and usually the events we care most in movies, web videos, surveillance video, etc. Event detection for video is an important problem in many applications, especially nowadays video become one of the most important information resource and watching videos become part of our everyday life. For example this technology can help us make index of key frames, which can be used by user to do content-based browsing (fast-forward to the moment they want to watch) [1].</p>

<p> You can read the following part of this page or read our <a href="final.pdf">report</a> for detail. </p>

<!--
<p><strong>Quick Jump:</strong></p>
<ol>
    <li><a href="#code">Code</a></li>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#duck">Why did my detector fail?</a></li>
    <li><a href="#falsepositives">Visualizing Top Detections</a></li>
    <li><a href="#whatdoeshogsee">What does HOG see?</a></li>
    <li><a href="#eyeglass">Eye Glass</a></li>
    <li><a href="#models">Visualizing Learned Models</a></li>
    <li><a href="#color">Recovering Color</a></li>
    <li><a href="#videos">Videos</a></li>
    <li><a href="#hoggles">HOGgles</a></li>
</ol>
-->
<br clear="all">

</div>

<div class="content">
<h2> Method </h2>
<table align="center"> 
    <tr>
    <td align="right"> <img src="f1.png"  height="65%" width="65%"> </td>
    <td align="left"> <img src="f2.png" height="65%" width="65%"> </td> 
    </tr>
</table>
<center> Example of extracted spatial-time interest points(STIP). </center>
<center> The figure shows that STIP can capture movement part of human.</center>
<p> We use trackelet feature[2]. The features are 256-dimensional spatial-time interest points(STIP) based on HOG and HOF(see report for detail). Since video's temporal and spatial properties vary between each other, the number of STIP extracted from each video also varies. The first thing is to project all the feature vector of different videos into same space. We clustering these spatial-temporal interest points to learn a dictionary (finally get 891 vocabularies or bases). Then calculate histogram of projected points in a video as its feature vector. We use these feature vectors to train SVM classifier for each action. Prediction is made by one-vs-all strategy. </p>
</div>

<div class="content">
<h2> Experiments </h2>
<p> The experiments are done on KTH action dataset, which contains 6 classes of actions with almost 100 videos for each class. We totally use 72 videos for training and 72 videos for test, the number of videos for different class is similar (11 to 12). We trained SVM with different kernels and variables, and linear model achieved the best performance, 86.1% accuracy on test set. Below is the confusion matrix. That shows good features' importance in classification, even simple classifier like linear SVM can work well with them. </p>
<center>
<table class="conf-mat">
    <tr> 
        <th> </th>
        <th>box </th>
        <th> handclap </th>
        <th> handwave </th>
        <th> joggling </th>
        <th> running </th>
        <th> walking </th> 
    </tr>
    <tr>
    <td class="header"> boxing</td>
    <td class="td10"> 10 </td>
    <td class="td1"> 1 </td>
    <td> 0 </td>
    <td> 0 </td>
    <td> 0 </td>
    <td class="td1"> 1 </td> 
    </tr>
    <tr>
    <td class="header"> handclapping</td>
    <td> 0</td>
    <td class="td12"> 12</td>
    <td> 0</td>
    <td> 0</td>
    <td> 0</td>
    <td> 0</td> 
    </tr>
    <tr>
    <td class="header"> handwaving</td>
    <td class="td1"> 1</td>
    <td> 0</td>
    <td class="td11"> 11</td>
    <td> 0</td>
    <td> 0</td>
    <td> 0</td> 
    </tr>
    <tr>
    <td class="header"> joggling</td>
    <td class="td1"> 1</td>
    <td> 0</td>
    <td> 0</td>
    <td class="td9"> 9</td>
    <td class="td2"> 2</td>
    <td> 0</td> 
    </tr>
    <tr>
    <td class="header"> running </td>
    <td class="td1"> 1</td>
    <td> 0</td>
    <td> 0</td>
    <td class="td2"> 2</td>
    <td class="td9"> 9</td>
    <td> 0</td> 
    </tr>
    <tr>
    <td class="header"> walking </td>
    <td class="td1"> 1</td>
    <td> 0</td>
    <td> 0</td>
    <td> 0</td>
    <td> 0</td>
    <td class="td11"> 11</td> 
    </tr>
</table>
</center>
</div>
<!--
<table class="contentblock">
<tr>
<td style="width:300px;">

<h2>Code</h2>
<p>We have released a fast and simple MATLAB function <code>invertHOG()</code> to invert HOG features. Usage is easy:</p>
<code>&gt;&gt; feat = features(im, 8);<br>
&gt;&gt; <strong>ihog = invertHOG(feat);</strong><br>
&gt;&gt; imagesc(ihog); 
</code>
<p>The above should invert any reasonably sized HOG feature in under a second on a modern desktop machine.</p>

<p>The code can be <a href="https://github.com/CSAILVision/ihog/archive/master.tar.gz">downloaded here</a> or you can checkout our <a href="https://github.com/CSAILVision/ihog">Github repository</a>. Installation is simple, but remember to read the <a href="https://github.com/CSAILVision/ihog/blob/master/README.md#ihog-inverting-histograms-of-oriented-gradients">README</a>.</p>

</td>
<td style="width:600px;">

<h2>Demo</h2>
<iframe src="http://saturday.csail.mit.edu:8080/?embed=1" id="demoframe"></iframe>

</td>
</tr>
</table>
-->
<!--
<div class="content" id="overview">

<h2>Overview</h2>

<p>This project introduces the tools to visualize feature spaces. Since most feature spaces are too high dimensional for humans
to directly inspect, we present algorithms to invert feature descriptors back to a natural image. We found that these
inversions provide an accurate and intuitive visualization of feature descriptors commonly used in object
detection. Below, we show an example of the visualization for HOG:

<table id="teasers">
<thead>
<tr>
<th>HOG [1]</th>
<th>Inverse (Us)</th>
<th>Original</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="results/teaser-hog.jpg"></td>
<td><img src="results/teaser-inverse.jpg"></td>
<td><img src="results/teaser-original.jpg"></td>
</tr>
<tr>
<td><img src="results/teaser2-hog.jpg"></td>
<td><img src="results/teaser2-inverse.jpg"></td>
<td><img src="results/teaser2-original.jpg"></td>
</tr>
<tr>
<td><img src="results/teaser3-hog.jpg"></td>
<td><img src="results/teaser3-inverse.jpg"></td>
<td><img src="results/teaser3-original.jpg"></td>
</tr>
<tr>
<td><img src="results/teaser4-hog.jpg"></td>
<td><img src="results/teaser4-inverse.jpg"></td>
<td><img src="results/teaser4-original.jpg"></td>
</tr>
</tbody>
</table>

</div>
-->
<!--
<div class="content" id="duck">

<h2>Why did my detector fail?</h2>

<p>Below we show a high scoring detection from an object detector with HOG features
and a linear SVM classifier trained on PASCAL. Why does our detector 
think that sea water looks like a car?</p>

<div style="text-align:center;">
<img src="results/000821_annotated.jpg.png">
</div>

<p>Our visualizations offer an explanation. Below we show the output from our visualization
on the HOG features for the false car detection. This visualization reveals that, while there are clearly 
no cars in the original image, there is a car hiding in the HOG descriptor.</p>

<div style="text-align:center;">
<img src="results/teaser_vis.png" width="500">
</div>

<p>HOG features see a slightly different visual world than what humans see, and by visualizing this space,
we can gain a more intuitive understanding of our object detectors. </p>

</div>
-->

<!--
<div class="content" id="models">
<h2>Visualizing Learned Models</h2>
<p>Our inverses allow us to visualize learned object models. Below we show a few deformable parts models. Notice
the structure that emerges with our visualization.</p>

<div style="text-align:center;">
<img height="100" src="models/car_4_parts.jpg">
<img height="100" src="models/person_2_parts.jpg">
<img height="100" src="models/bottle_6_parts.jpg">
<img height="100" src="models/bicycle_2_parts.jpg">
<img height="100" src="models/motorbike_2_parts.jpg">
<img height="100" src="models/pottedplant_4_parts.jpg">
<img height="100" src="models/train_4_parts.jpg">
<img height="100" src="models/bus_2_parts.jpg">
<img height="100" src="models/horse_2_parts.jpg">
<img height="100" src="models/tvmonitor_4_parts.jpg">
<img height="100" src="models/chair_4_parts.jpg">
</div>

<p>First row: car, person, bottle, bicycle, motorbike, potted plant. Second row: train, bus, horse, television, chair.</p>

</div>
-->

<div class="content">

<h2>References</h2>
<ol>
<li> Laptev, I., Marszalek, M., Schmid, C., & Rozenfeld, B. (2008, June). Learning realistic human actions from movies. In <em>Computer Vision and Pattern Recognition</em>, 2008. CVPR 2008. <em>IEEE Conference</em> on (pp. 1-8). IEEE.</li>
<li> Raptis, M., & Soatto, S. (2010). Tracklet descriptors for action modeling and video analysis. In <em>Computer Vision–ECCV</em> 2010 (pp. 577-590). Springer Berlin Heidelberg. </li>
</ol>

</div>

<div id="toolBackTo" class="back-to" style="display: none">
<a class="back-top" href="#" title="Back to Top">Back to Top</a>
</div>

<script type="text/javascript">
var glide =new function(){
    function $id(id){return document.getElementById(id);};
    this.layerGlide=function(hSlider,oSlider,sSingleSize,fSpeed,point){
        var interval,oslideRange;
        var time=1; 
        //var speed = 0.1;
        var speed = fSpeed; 
        var setValLeft=function(s){
            return function(){
                oslideRange = Math.abs(parseInt($id(oSlider).style[point]));    
                $id(oSlider).style[point] =-Math.floor(oslideRange+(parseInt(s*sSingleSize) - oslideRange)*speed) +'px';        
                if(oslideRange==[sSingleSize * s]){
                    clearInterval(interval);
                    //a=s;
                }
            }
        };
        
        $id(hSlider).onmouseover = function(){
                clearInterval(interval);
                interval = setInterval(setValLeft(1),time);     
            }
        
        $id(hSlider).onmouseout = function(){
                clearInterval(interval);
                interval = setInterval(setValLeft(0),time);
            }
    }
}
glide.layerGlide('slide-show','show',278,0.1,'left');
glide.layerGlide('slide-show2','show2',270,0.1,'left');
</script>


<script type="text/javascript">

    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-7870337-1']);
    _gaq.push(['_setDomainName', 'none']);
    _gaq.push(['_setAllowLinker', true]);
    _gaq.push(['_trackPageview']);

    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

</script>

<!-- Start Quantcast tag -->
<script type="text/javascript">
    _qoptions={
        qacct:"p-0dYLvhSGGqUWo"
    };
</script>
<script type="text/javascript" src="//edge.quantserve.com/quant.js"></script>
<noscript>
<img src="//pixel.quantserve.com/pixel/p-0dYLvhSGGqUWo.gif" style="display: none;" border="0" height="1" width="1" alt="Quantcast"/>
</noscript>
<!-- End Quantcast tag -->

<script>

    (function(jQuery) {
        try {
            if (jQuery) {
                jQuery('div.blog-social div.fb-like').attr('class', 'blog-social-item blog-fb-like');
                jQuery('#commentArea iframe').css('min-height', '410px');
                if (jQuery('.product-button').length > 0){
                    jQuery(document).ready(function(){
                        jQuery('.product-button').parent().each(function(index, product){
                            if(jQuery(product).attr('target') == 'paypal'){
                                if (!jQuery(product).find('> [name="bn"]').length){
                                    jQuery('<input>').attr({
                                        type: 'hidden',
                                        name: 'bn',
                                        value: 'DragAndDropBuil_SP_EC'
                                    }).appendTo(product);
                                }
                            }
                        });
                        jQuery.ajax({
                            type: "GET",
                            url: "/ajax/apps/stats.php?stat=sites.commerce.legacy.visit"
                        });
                    });
                    jQuery('.product-button').click(function() {
                        var val = jQuery(this.form).serialize();
                        jQuery.ajax({
                            type: "POST",
                            url: "/ajax/apps/stats.php?stat=sites.commerce.legacy.addcart",
                            data: { values: val, host: window.location.host }
                        });
                    });
                }
            }
            else {
                // Prototype
                $$('div.blog-social div.fb-like').each(function(div) {
                    div.className = 'blog-social-item blog-fb-like';
                });
                $$('#commentArea iframe').each(function(iframe) {
                    iframe.style.minHeight = '410px';
                });
            }
        }
        catch(ex) {}
    })(window._W && _W.jQuery);

</script>
</body>
</html>
